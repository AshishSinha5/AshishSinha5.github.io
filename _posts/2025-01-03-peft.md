# Parameter Efficient Fine Tuning (PEFT) – Adapting Large Models at Scale

## Introduction

Large-scale neural networks, particularly transformer-based architectures, have become the dominant paradigm for representation learning in language, vision, and multimodal systems. While pre-training on large corpora enables strong general-purpose representations, downstream adaptation remains challenging due to the computational and memory cost of full fine tuning.

Parameter Efficient Fine Tuning (PEFT) methods address this challenge by enabling task adaptation through training a small subset of parameters while keeping the majority of the pre-trained model frozen. This approach significantly reduces training cost while preserving the representational power of the base model.

This post presents a detailed technical discussion of PEFT methods, their mathematical formulation, and architectural implications.

---

## Limitations of Full Fine Tuning

Let a pre-trained model be parameterized by \(\theta \in \mathbb{R}^N\), where \(N\) is typically in the order of billions.

Full fine tuning requires:
- Gradient computation and storage for all parameters
- Optimizer state memory proportional to model size
- Large-scale distributed training infrastructure

More importantly, full fine tuning often results in:
- Redundant parameter updates
- Overfitting on smaller downstream datasets
- Catastrophic forgetting of pre-trained knowledge

Empirical studies show that downstream tasks frequently require only localized modifications to pre-trained representations rather than global re-optimization.

---

## Formal Definition of PEFT

PEFT decomposes the model parameters as:

\[
\theta = \theta_{\text{frozen}} \cup \theta_{\text{adapt}}
\]

where:
- \(\theta_{\text{frozen}}\) remains unchanged during training
- \(\theta_{\text{adapt}}\) is a small, trainable subset

The optimization objective becomes:

\[
\min_{\theta_{\text{adapt}}} \mathcal{L}(f(x; \theta_{\text{frozen}}, \theta_{\text{adapt}}), y)
\]

This formulation restricts learning to a low-dimensional subspace of the full parameter space.

---

## Adapter-Based Fine Tuning

### Architectural Placement

Adapters are lightweight neural modules inserted within transformer layers, typically after the feedforward network or attention block.

### Mathematical Formulation

Given a hidden representation \(h \in \mathbb{R}^d\), an adapter computes:

\[
h_{\text{adapter}} = h + W_{\text{up}} \sigma(W_{\text{down}} h)
\]

where:
- \(W_{\text{down}} \in \mathbb{R}^{r \times d}\)
- \(W_{\text{up}} \in \mathbb{R}^{d \times r}\)
- \(r \ll d\)

Only \(W_{\text{down}}\) and \(W_{\text{up}}\) are trainable.

### Characteristics

- Preserves the original network depth
- Enables task-specific modularity
- Introduces minor inference latency due to additional layers

Adapters are particularly useful in multi-task and continual learning settings.

---

## Low-Rank Adaptation (LoRA)

### Motivation

Transformer layers rely heavily on large linear projections. Updating these matrices directly is inefficient. LoRA constrains updates to a low-rank subspace.

### Mathematical Decomposition

For a weight matrix \(W \in \mathbb{R}^{d \times d}\), the adapted weight is:

\[
W' = W + BA
\]

with:
- \(A \in \mathbb{R}^{r \times d}\)
- \(B \in \mathbb{R}^{d \times r}\)

The rank \(r\) controls expressiveness and parameter efficiency.

### Application in Transformers

LoRA is typically applied to:
- Query, Key, Value projections
- Output projection in multi-head attention

The forward pass becomes:

\[
\text{Attention}(Q + \Delta Q, K, V)
\]

where \(\Delta Q = B_q A_q X\).

### Practical Properties

- No additional inference cost after weight merging
- Compatible with mixed-precision training
- Supports multiple task-specific adapters on the same backbone

---

## Prompt and Prefix Tuning

### Prompt Tuning

Prompt tuning optimizes a set of learnable embeddings prepended to the input sequence.

\[
X' = [P; X]
\]

Only the prompt parameters \(P\) are trained.

### Prefix Tuning

Prefix tuning extends this idea by injecting learned key-value pairs into each attention layer.

\[
K' = [K_{\text{prefix}}; K], \quad V' = [V_{\text{prefix}}; V]
\]

This approach influences attention distributions without modifying token embeddings directly.

### Limitations

- Less effective for tasks requiring deep structural adaptation
- Sensitive to prompt length and initialization

---

## Bias and Normalization Fine Tuning

An extreme PEFT approach involves training only:
- Bias parameters
- LayerNorm scale and shift parameters

Despite minimal parameter updates, this method can alter activation distributions across layers, leading to non-trivial performance gains.

---

## Theoretical Perspective

PEFT methods exploit two properties of large neural networks:

1. Redundancy in parameterization  
2. Local linearity in the loss landscape near pre-trained optima  

By restricting optimization to a low-dimensional manifold, PEFT achieves efficient task adaptation without disrupting global representations.

---

## Comparison of PEFT Methods

| Method | Trainable Params | Expressiveness | Inference Overhead |
|------|------------------|----------------|--------------------|
| Adapters | Low | High | Low |
| LoRA | Very Low | High | None |
| Prefix Tuning | Extremely Low | Medium | None |
| Bias Only | Minimal | Low | None |

---

## Practical Considerations

PEFT should be preferred when:
- Model size exceeds available training memory
- Multiple downstream tasks share a common backbone
- Rapid experimentation is required
- Deployment constraints prohibit model duplication

In production systems, PEFT also simplifies versioning and rollback by decoupling base models from task-specific adaptations.

---

## Conclusion

Parameter Efficient Fine Tuning enables scalable adaptation of large models by restricting optimization to a small, structured subset of parameters. As model sizes continue to grow, PEFT techniques are increasingly fundamental to practical machine learning systems.

A strong understanding of PEFT is essential for designing efficient and maintainable large-scale model pipelines.

## References

1. Houlsby, N., Giurgiu, A., Jastrzębski, S., Morrone, B., de Laroussilhe, Q., Gesmundo, A., Attariyan, M., & Gelly, S. (2019).  
   *Parameter-Efficient Transfer Learning for NLP*.  
   Proceedings of the 36th International Conference on Machine Learning (ICML).  
   https://arxiv.org/abs/1902.00751

2. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, L., & Chen, W. (2021).  
   *LoRA: Low-Rank Adaptation of Large Language Models*.  
   International Conference on Learning Representations (ICLR).  
   https://arxiv.org/abs/2106.09685

3. Li, X. L., & Liang, P. (2021).  
   *Prefix-Tuning: Optimizing Continuous Prompts for Generation*.  
   Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL).  
   https://arxiv.org/abs/2101.00190

4. Lester, B., Al-Rfou, R., & Constant, N. (2021).  
   *The Power of Scale for Parameter-Efficient Prompt Tuning*.  
   Empirical Methods in Natural Language Processing (EMNLP).  
   https://arxiv.org/abs/2104.08691

5. Zaken, E. B., Goldberg, Y., & Ravfogel, S. (2022).  
   *BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language Models*.  
   Proceedings of ACL.  
   https://arxiv.org/abs/2106.10199

6. He, X., Wang, J., & Chen, X. (2022).  
   *Towards a Unified View of Parameter-Efficient Transfer Learning*.  
   International Conference on Learning Representations (ICLR).  
   https://arxiv.org/abs/2110.04366

7. Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023).  
   *QLoRA: Efficient Finetuning of Quantized LLMs*.  
   Advances in Neural Information Processing Systems (NeurIPS).  
   https://arxiv.org/abs/2305.14314
